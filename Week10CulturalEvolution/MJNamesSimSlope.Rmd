---
title: "Can A Neutral Model Explain Baby Name Inheritance?"
author: "Michael Jordan"
date: "2023-12-05"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, error=FALSE, warning=FALSE, message=FALSE}
# Clear the workspace and load libraries.
rm(list=ls())
library(babynames)
library(dplyr)
library(ggplot2)
library(scales)

# Set simulation parameters: 1) base year, 2) number of simulation reps, 3)
# number of years after base year to create, 4) weighting factor in weighted
# model, 5) perc of top/bottom names to have their weight adjusted.
base.year <- 1920
n.reps <- 100
n.years <- 30
wt <- .05
perc <- .1
```
In this week's module, we reviewed the proposal that so-called "neutral" models, in which trait inheritance is stochastic, could explain outcomes that evolutionary biology has long held are due to the pressure of selective fitness.  If neutral models can explain the data, why bother with a complicated theory when a simple stochastic process will do?  Occam's razor, etc.

Of course, fitting a model is not in-and-of-itself proof positive of a particular causal mechanism.  Numerous studies, such as the Lenski study of *e. coli* evolution, have demonstrated that traits do not develop randomly in a population, but instead develop in response to selective pressure operating on the population. Nevertheless, neutral models have come into vogue in recent years and proponents have claimed that they can explain outcomes in a wide variety of processes that common sense tells us should involve some degree of selective pressure.

In this practical, we were tasked with proposing and falsify a simple neutral model using just such a process: naming newborn babies. Do new parents typically pluck a name for their child out of a hat (as a neutral model might suggest)? Or, are some names more heritable than others based on factors such as fads, cultural traditions, social movements, etc. (i.e., do they come under selective pressure)?  To test this, we will take a data set of over one hundred years of baby names in the US, describe a parameter of the data set, and compare this parameter in the real world data to the results of simulations of name inheritance that use a neutral model.  By comparing the results of the simulations of the neutral model to the real world data, we can evaluate whether the real world data could plausibly have been the result of the neutral model.

First, let's explore our real world data.  We'll examine only female names to avoid confusion around names that occur for both sexes.

```{r explore_pop, echo=FALSE, out.width="75%", out.height="75%", fig.align='center', error=FALSE, warning=FALSE, message=FALSE}
# Capture female baby names.
b <- babynames[babynames$sex == "F", ]

# Derive population over time and plot
pop <- dplyr::summarize(dplyr::group_by(b, year), pop_size = sum(n))
plot(pop$pop_size ~ pop$year, xlab = "Year", ylab = "Female Population")
```
As expected, population varies over time.  We could use any time period for our simulation.  But, let's start at `r base.year`.  It's a bit arbitrary, but it's around a time when population was stable, it's in the modern era (not sure how accurate 19th century record keeping was), it will allow us to test over a long time period, and frankly a somewhat lower initial name count is preferable to keep our loops from being too computationally costly (I'd rather this thing not run for hours).

Next we need to decide what population parameter we'd like to test.  One dynamic that might be interesting to look at is survivorship. Among female names in `r base.year`, what percentage were inherited in the following years? Let's look at that in the real data over a `r n.years` year period (I'd like to do more years, but this is again so that my computer doesn't explode).

```{r explore_surv, echo=FALSE, out.width="75%", out.height="75%", fig.align='center', error=FALSE, warning=FALSE, message=FALSE}

# Filter for just names in base year.
b.base <- b[b$year == base.year, ]

# Define a function to calculate survivorship for a given year vs base year.
CalcSurv <- function(x) {sum(b.base$name %in% x) / nrow(b.base)}

# Calculate survivorship over time for real data.
surv.r <- b[b$year > base.year & b$year <= (base.year + n.years), c(1, 3)] %>%
  group_by(year) %>%
  summarize(survivorship = CalcSurv(name))
plot(surv.r$year, surv.r$survivorship, xlab = "Year", ylab = "Survivorship")
```
The relationship looks more or less linear, so let's fit a linear model to it and extract the slope.

```{r explore_surv_mod, echo=FALSE, out.width="75%", out.height="75%", fig.align='center'}
# Fit a linear model to the data, plot it, and extract its slope
plot(surv.r$year, surv.r$survivorship, xlab = "Year", ylab = "Survivorship")
surv.m <- lm(data = surv.r, survivorship ~ year)
abline(surv.m)
slope.r <- surv.m$coefficients[2][[1]]
```

The R-squared of the model is great, `r round(summary(surv.m)$r.squared, digits = 3)`, so a linear model fits the data reasonably well. The slope of the model is `r round(slope.r, digits = 3)`.

The slope of a linear model fit on survivorship over time might be an interesting parameter to test. If there is selective pressure on names, we might expect survivorship of a neutral model to decrease at a faster or slower rate than we see in the actual population.  It's a reasonable supposition...let's go with it!

Now, we need to pick a neutral model to simulate.  We'll keep it simple.  We only care about survivorship of names in `r base.year`, so we'll begin with all names in `r base.year` in proportion to their popularity that year.  To simulate name inheritance in 1921, we'll randomly sample a number of names from the `r base.year` name pool equal to the 1921 real world population.  We'll continue to sample in the same way, iteratively sampling from the name pool in the prior year, for `r n.years` years following `r base.year`. This is a very simple and of course unrealistic model (it assumes that parents have knowledge of all names in the prior year). But, it is neutral since inheritance is purely stochastic and not impacted by selective pressure.

So, we're all set.  Now all we need to do is run our simulation `r n.reps` times, fit a linear model to each run, extract the slope from each model, and plot a histogram of slopes to construct a distribution of slopes to ascertain the range of slopes our neutral model is expected to produce.  This is analogous to taking a sample from a population, estimating the sample distribution based on the count and standard deviation of the sample, and evaluating a claim about a test statistic based on whether or not it falls within the hypothetical sample distribution.  The main difference is there here we actually derive our sample distribution out of simulation runs!  If the slope of the model fit on real world survivorship falls within the histogram of slopes of simulation runs of our neutral model, then we will believe that the neutral model could explain the real world data.  If it does not, we will say that it is unlikely that the neutral model could have produced the real world data.

```{r simulate_one, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
# Create a "name pool" for the base year where each name appears with the same
# frequency it did in the real world population.
names.s <- c()
for (i in 1:nrow(b.base)) {
  temp <- rep(b.base[[i, 3]], each = b.base[[i, 4]])
  names.s <- append(names.s, temp)
}

# Create global objects for the simulation to act upon.
# 1) A DF to contain the results for each year of a given simulation run. This
# will contain the year, sampled names, and survivorship.
# 2) An empty list to store each simulation run.
# 3) An empty DF to store the slope of the linear model fitted on each
# simulation run.
seed <- data.frame(
  year = base.year,
  data = I(list(names.s)),
  surv = 1)
complete.runs = list()
slopes <- data.frame(run = integer(), slope = integer())

# Simulate the neutral model per the base year, number of years, and number of
# simulation reps set above.
for (i in 1:n.reps) {
  run.hist <- seed
  for (j in 1:n.years) {
    smp <- sample(run.hist[[j, 2]], pop[[j + 1, 2]], replace = TRUE)
    run.hist.row <- data.frame(
      year = j + base.year,
      data = I(list(smp)),
      surv = CalcSurv(smp))
    run.hist <- rbind(run.hist, run.hist.row)
    if(j == n.years) {complete.runs[[i]] <- run.hist[-1, ]}
  }
}

# For each simulation run, fit a linear model, extract the slope, and save it to
# the slope vector.
for (i in 1:n.reps) {
  temp <- data.frame(
    run = i,
    slope = lm(
      complete.runs[[i]]$surv ~ complete.runs[[i]]$year,
      data = complete.runs[[i]])$coefficients[[2]])
  slopes <- rbind(slopes, temp)
}

# Plot a histogram of the slopes for all runs of the simulation vs the slope
# of the actual population.
ggplot2::ggplot(slopes, aes(x = slope)) +
  ggplot2::geom_histogram() +
  ggplot2::geom_point(aes(x = slope.r, y = 0), colour = "blue", size = 6, shape = 8)
```

What do you know?  The real world slope fell far outside of the histogram of simulation runs! Specifically, it was far to the right of the simulations, meaning that the neutral model produced results in which survivorship declined much faster than we see in reality.  What could explain that?

One possibility is that parents might have a preference for some names over others (i.e., that selective pressure acts on name inheritence!).  For example, perhaps parents tend to prefer rare names and avoid common names. Who wants to be like everyone else? At some point, [how many Olivias](https://www.ssa.gov/oact/babynames/top5names.html) can there be?

To test this, instead of simulating a truly neutral model, let's simulate a model in which some names are fitter than others. This is arbitrary, but let's say that the top `r scales::percent(perc)` of names by frequency in any given year get a `r scales::percent(wt)` downgrade in their likelihood of being inherited and the bottom `r scales::percent(perc)` get a `r scales::percent(wt)` boost. The model will otherwise be identical.  We'll still start in `r base.year`, sample each year from the names of the previous run, and run it for `r n.years` years across `r n.reps` simulation runs.

What happens if we run a model like this?

```{r simulate_two, echo=FALSE, error=FALSE, warning=FALSE, message=FALSE}
# Define a function that takes a vector of names and returns a vector of weights
# of equal length.  Weighting is according to frequency of a given  element
# within the name vector, with the weights of the top 10% of names by frequency
# downgraded by 10% and the weights of the bottom 10% of names by frequency
# upgraded by 10%.
CreateWeightsVector <- function(x) {
  weights.by.name <- data.frame(names = x) %>%
    dplyr::group_by(names) %>%
    dplyr::summarize(count = n()) %>%
    dplyr::arrange(desc(count)) %>%
    dplyr::mutate(
      weight = ifelse(
        row_number() <= round(length(unique(x)) / 10),
        1 / length(unique(x)) * (1 - wt),
        ifelse(
          row_number() >= length(unique(x)) - round(length(unique(x)) / 10) + 1,
          1 / length(unique(x)) * (1 + wt),
          1 / length(unique(x)))
      ))
  weight.vector <- left_join(
    data.frame(names = x),
    weights.by.name[, c(1, 3)],
    by = "names")[, 2]
  return(weight.vector)
}

# Just as in simulation_one, create 3 global objects to store simulation output.
# The only addition is an additional column for weights in the seed object.
seed.w <- data.frame(
  year = base.year,
  data = I(list(names.s)),
  weights = I(list(CreateWeightsVector(names.s))),
  surv = 1)
complete.runs.w = list()
slopes.w <- data.frame(run = integer(), slope = integer())

# Rerun simulation with weights.
for (i in 1:n.reps) {
  run.hist <- seed.w
  for (j in 1:n.years) {
    smp <- sample(
      run.hist[[j, 2]],
      pop[[j + 1, 2]],
      prob = run.hist[[j, 3]],
      replace = TRUE)
    run.hist.row <- data.frame(
      year = j + base.year,
      data = I(list(smp)),
      weights = I(list(CreateWeightsVector(smp))),
      surv = CalcSurv(smp))
    run.hist <- rbind(run.hist, run.hist.row)
    if(j == n.years) {complete.runs.w[[i]] <- run.hist[-1, ]}
  }
}

# Fit linear models.
for (i in 1:n.reps) {
  temp <- data.frame(
    run = i,
    slope = lm(
      complete.runs.w[[i]]$surv ~ complete.runs.w[[i]]$year,
      data = complete.runs.w[[i]])$coefficients[[2]])
  slopes.w <- rbind(slopes.w, temp)
}

# Plot output.
ggplot(slopes.w, aes(x = slope)) +
  geom_histogram() +
  geom_point(aes(x = slope.r, y = 0), colour = "blue", size = 6, shape = 8)

# Evaluate whether real slope is within 96% confidence intervals
in.p <- slope.r >= arrange(
  slopes.w, slope)[3, 2] & slope.r <= arrange(slopes.w, slope)[97, 2]

# Outcome text for the two possibilities
# TODO: Re-write both of these.
in.p.t <- "As you can see, now that we have given a positive weight to uncommon names in our simulation, the histogram of slopes is to the right of where the distribution was in the first simulation.  This makes sense! Giving uncommon names a boost in fitness slows down the rate at which they go extinct.  More importantly, now the real slope of a linear model fit through survivorship falls within ~ 95% of the simulation run slopes.  This suggests that this model in which inheritance is not neutral could have generated the data we observe in the real world. This is evidence that parents do not pick their baby's name stochastically, but instead on some basis which results in some names having more fitness and some names less.  In other words, a neutral model does not hold."

not.in.p.t <- "As you can see, now that we have given a positive weight to uncommon names in our simulation, the histogram of slopes is to the right of where the distribution was in the first simulation.  This makes sense! Giving uncommon names a boost in fitness slows down the rate at which names go extinct.  Although, just as in the first model, this model is not statistically significant.  But, perhaps if we were to develop a more sophisticated understanding of how inheritance might be functioning in the real world data, we could build an inheritance model that produces simulated results in line with what we see in reality."
```

`r ifelse(in.p, in.p.t, not.in.p.t)`